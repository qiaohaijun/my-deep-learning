1. Ruder, An overview of gradient descent optimization algorithms http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms
2. https://climin.readthedocs.org/en/latest/#optimizer-overview
3. Schaul, Antonoglou, Silver, Unit Tests for Stochastic Optimization http://arxiv.org/pdf/1312.6055v3.pdf
4. Sutskever, Martens, Dahl, and Hinton, “On the importance of initialization and momentum in deep learning” (ICML 2013)
5. Dyer, “Notes on AdaGrad”
6. Duchi, Hazan, and Singer, “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization” (COLT 2010)
7. Hinton, Srivastava, and Swersky, “rmsprop: Divide the gradient by a running average of its recent magnitude”
8. , 12) Dauphin, Vries, Chung and Bengion, “RMSProp and equilibrated adaptive learning rates for non-convex optimization”
9. Graves, “Generating Sequences with Recurrent Neural Networks”
10. Zeiler, “Adadelta: An Adaptive Learning Rate Method”
11. Kingma and Ba, “Adam: A Method for Stochastic Optimization”
13. Gulcehre and Bengio, “Adasecant: Robust Adaptive Secant Method for Stochastic Gradient”
14. Schaul, Zhang, LeCun, “No More Pesky Learning Rates”
15. Riedmiller and Bruan, “A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm”
