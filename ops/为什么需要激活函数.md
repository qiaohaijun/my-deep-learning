<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

为了避免文件名字太长，所以切断了。

问题是，神经网络为什么需要非线性激活函数，为什么relu 能够避免梯度消失。

---

## 第一个问题，为什么要引入激活函数
注意这里有定语，**非线性**

如果不用激励函数，其实就是相当于激活函数是
$$f(x)=x$$


## 为什么使用relu
